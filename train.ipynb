{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch._six'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m set_up_data\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_cpu_stats_over_ranks\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtrain_helpers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m set_up_hyperparams, load_vaes, load_opt, accumulate_stats, save_model, update_ema\n",
      "File \u001b[1;32mc:\\Users\\sontu\\OneDrive\\US_study_abroad\\University_of_Michigan\\CLASS\\Fall_2024\\EECS_553\\Final_project\\vdvae\\train_helpers.py:18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcontextlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m contextmanager\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdist\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mapex\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FusedAdam \u001b[38;5;28;01mas\u001b[39;00m AdamW\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvae\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VAE\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparallel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DistributedDataParallel\n",
      "File \u001b[1;32mc:\\Users\\sontu\\anaconda3\\envs\\vdvae\\lib\\site-packages\\apex\\__init__.py:10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdistributed\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parallel\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m amp\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fp16_utils\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# For optimizers and normalization there is no Python fallback.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Absence of cuda backend is a hard error.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# I would like the errors from importing fused_adam_cuda or fused_layer_norm_cuda\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# available (for example because they built improperly in a way that isn't revealed until\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# load time) the error message is timely and visible.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sontu\\anaconda3\\envs\\vdvae\\lib\\site-packages\\apex\\amp\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mamp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m init, half_function, float_function, promote_function,\\\n\u001b[0;32m      2\u001b[0m     register_half_function, register_float_function, register_promote_function\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhandle\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m scale_loss, disable_casts\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfrontend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m initialize, state_dict, load_state_dict\n",
      "File \u001b[1;32mc:\\Users\\sontu\\anaconda3\\envs\\vdvae\\lib\\site-packages\\apex\\amp\\amp.py:5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlists\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functional_overrides, torch_overrides, tensor_overrides\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_amp_state\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _amp_state\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfrontend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sontu\\anaconda3\\envs\\vdvae\\lib\\site-packages\\apex\\amp\\frontend.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_initialize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _initialize\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_amp_state\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _amp_state, warn_or_err, maybe_print\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OrderedDict\n",
      "File \u001b[1;32mc:\\Users\\sontu\\anaconda3\\envs\\vdvae\\lib\\site-packages\\apex\\amp\\_initialize.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_six\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m string_classes\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch._six'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import imageio\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from data import set_up_data\n",
    "from utils import get_cpu_stats_over_ranks\n",
    "from train_helpers import set_up_hyperparams, load_vaes, load_opt, accumulate_stats, save_model, update_ema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(H, data_input, target, vae, ema_vae, optimizer, iterate):\n",
    "    t0 = time.time()\n",
    "    vae.zero_grad()\n",
    "    stats = vae.forward(data_input, target)\n",
    "    stats['elbo'].backward()\n",
    "    grad_norm = torch.nn.utils.clip_grad_norm_(vae.parameters(), H.grad_clip).item()\n",
    "    distortion_nans = torch.isnan(stats['distortion']).sum()\n",
    "    rate_nans = torch.isnan(stats['rate']).sum()\n",
    "    stats.update(dict(rate_nans=0 if rate_nans == 0 else 1, distortion_nans=0 if distortion_nans == 0 else 1))\n",
    "    stats = get_cpu_stats_over_ranks(stats)\n",
    "\n",
    "    skipped_updates = 1\n",
    "    # only update if no rank has a nan and if the grad norm is below a specific threshold\n",
    "    if stats['distortion_nans'] == 0 and stats['rate_nans'] == 0 and (H.skip_threshold == -1 or grad_norm < H.skip_threshold):\n",
    "        optimizer.step()\n",
    "        skipped_updates = 0\n",
    "        update_ema(vae, ema_vae, H.ema_rate)\n",
    "\n",
    "    t1 = time.time()\n",
    "    stats.update(skipped_updates=skipped_updates, iter_time=t1 - t0, grad_norm=grad_norm)\n",
    "    return stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_step(data_input, target, ema_vae):\n",
    "    with torch.no_grad():\n",
    "        stats = ema_vae.forward(data_input, target)\n",
    "    stats = get_cpu_stats_over_ranks(stats)\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_for_visualization(data, preprocess_fn, num, dataset):\n",
    "    for x in DataLoader(data, batch_size=num):\n",
    "        break\n",
    "    orig_image = (x[0] * 255.0).to(torch.uint8).permute(0, 2, 3, 1) if dataset == 'ffhq_1024' else x[0]\n",
    "    preprocessed = preprocess_fn(x)[0]\n",
    "    return orig_image, preprocessed\n",
    "\n",
    "def write_images(H, ema_vae, viz_batch_original, viz_batch_processed, fname, logprint):\n",
    "    zs = [s['z'].cuda() for s in ema_vae.forward_get_latents(viz_batch_processed)]\n",
    "    batches = [viz_batch_original.numpy()]\n",
    "    mb = viz_batch_processed.shape[0]\n",
    "    lv_points = np.floor(np.linspace(0, 1, H.num_variables_visualize + 2) * len(zs)).astype(int)[1:-1]\n",
    "    for i in lv_points:\n",
    "        batches.append(ema_vae.forward_samples_set_latents(mb, zs[:i], t=0.1))\n",
    "    for t in [1.0, 0.9, 0.8, 0.7][:H.num_temperatures_visualize]:\n",
    "        batches.append(ema_vae.forward_uncond_samples(mb, t=t))\n",
    "    n_rows = len(batches)\n",
    "    im = np.concatenate(batches, axis=0).reshape((n_rows, mb, *viz_batch_processed.shape[1:])).transpose([0, 2, 1, 3, 4]).reshape([n_rows * viz_batch_processed.shape[1], mb * viz_batch_processed.shape[2], 3])\n",
    "    logprint(f'printing samples to {fname}')\n",
    "    imageio.imwrite(fname, im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(H, data_train, data_valid, preprocess_fn, vae, ema_vae, logprint):\n",
    "    optimizer, scheduler, cur_eval_loss, iterate, starting_epoch = load_opt(H, vae, logprint)\n",
    "    train_sampler = DistributedSampler(data_train, num_replicas=H.mpi_size, rank=H.rank)\n",
    "    viz_batch_original, viz_batch_processed = get_sample_for_visualization(data_valid, preprocess_fn, H.num_images_visualize, H.dataset)\n",
    "    early_evals = set([1] + [2 ** exp for exp in range(3, 14)])\n",
    "    stats = []\n",
    "    iters_since_starting = 0\n",
    "    H.ema_rate = torch.as_tensor(H.ema_rate).cuda()\n",
    "    for epoch in range(starting_epoch, H.num_epochs):\n",
    "        train_sampler.set_epoch(epoch)\n",
    "        for x in DataLoader(data_train, batch_size=H.n_batch, drop_last=True, pin_memory=True, sampler=train_sampler):\n",
    "            data_input, target = preprocess_fn(x)\n",
    "            training_stats = training_step(H, data_input, target, vae, ema_vae, optimizer, iterate)\n",
    "            stats.append(training_stats)\n",
    "            scheduler.step()\n",
    "            if iterate % H.iters_per_print == 0 or iters_since_starting in early_evals:\n",
    "                logprint(model=H.desc, type='train_loss', lr=scheduler.get_last_lr()[0], epoch=epoch, step=iterate, **accumulate_stats(stats, H.iters_per_print))\n",
    "\n",
    "            if iterate % H.iters_per_images == 0 or (iters_since_starting in early_evals and H.dataset != 'ffhq_1024') and H.rank == 0:\n",
    "                write_images(H, ema_vae, viz_batch_original, viz_batch_processed, f'{H.save_dir}/samples-{iterate}.png', logprint)\n",
    "\n",
    "            iterate += 1\n",
    "            iters_since_starting += 1\n",
    "            if iterate % H.iters_per_save == 0 and H.rank == 0:\n",
    "                if np.isfinite(stats[-1]['elbo']):\n",
    "                    logprint(model=H.desc, type='train_loss', epoch=epoch, step=iterate, **accumulate_stats(stats, H.iters_per_print))\n",
    "                    fp = os.path.join(H.save_dir, 'latest')\n",
    "                    logprint(f'Saving model@ {iterate} to {fp}')\n",
    "                    save_model(fp, vae, ema_vae, optimizer, H)\n",
    "\n",
    "            if iterate % H.iters_per_ckpt == 0 and H.rank == 0:\n",
    "                save_model(os.path.join(H.save_dir, f'iter-{iterate}'), vae, ema_vae, optimizer, H)\n",
    "\n",
    "        if epoch % H.epochs_per_eval == 0:\n",
    "            valid_stats = evaluate(H, ema_vae, data_valid, preprocess_fn)\n",
    "            logprint(model=H.desc, type='eval_loss', epoch=epoch, step=iterate, **valid_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(H, ema_vae, data_valid, preprocess_fn):\n",
    "    stats_valid = []\n",
    "    valid_sampler = DistributedSampler(data_valid, num_replicas=H.mpi_size, rank=H.rank)\n",
    "    for x in DataLoader(data_valid, batch_size=H.n_batch, drop_last=True, pin_memory=True, sampler=valid_sampler):\n",
    "        data_input, target = preprocess_fn(x)\n",
    "        stats_valid.append(eval_step(data_input, target, ema_vae))\n",
    "    vals = [a['elbo'] for a in stats_valid]\n",
    "    finites = np.array(vals)[np.isfinite(vals)]\n",
    "    stats = dict(n_batches=len(vals), filtered_elbo=np.mean(finites), **{k: np.mean([a[k] for a in stats_valid]) for k in stats_valid[-1]})\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test_eval(H, ema_vae, data_test, preprocess_fn, logprint):\n",
    "    print('evaluating')\n",
    "    stats = evaluate(H, ema_vae, data_test, preprocess_fn)\n",
    "    print('test results')\n",
    "    for k in stats:\n",
    "        print(k, stats[k])\n",
    "    logprint(type='test_loss', **stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H, logprint = set_up_hyperparams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H, data_train, data_valid_or_test, preprocess_fn = set_up_data(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae, ema_vae = load_vaes(H, logprint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if H.test_eval:\n",
    "    run_test_eval(H, ema_vae, data_valid_or_test, preprocess_fn, logprint)\n",
    "else:\n",
    "    train_loop(H, data_train, data_valid_or_test, preprocess_fn, vae, ema_vae, logprint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vdvae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
